{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd009d18-60cd-402e-bccf-fd8878c43c47",
   "metadata": {},
   "source": [
    "# Decision Trees, Ensemble Methods and Hyperparameter tuning workshop:\n",
    "\n",
    "# 3rd November 2022\n",
    "\n",
    "![title](images/pydata_cardiff.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235fafd-6ab1-4bf4-8116-a138b1a4d2e3",
   "metadata": {},
   "source": [
    "## Outline of the workshop\n",
    "\n",
    "This notebook will hopefully provide a simple outline of how classification models can be created using Decision Trees: either single trees or groups of trees in an \"ensemble model\". The following key points are raised:\n",
    "\n",
    "* What is a decision tree?\n",
    "* How is it used to build a classification model?\n",
    "* How the results of multiple decision trees can be combined to create very powerful and accurate models\n",
    "    * An explanation of the term: \"Ensembles of __weak learners__\"\n",
    "* A description of the hyper-parameters, which effects how trees are built, is provided\n",
    "* An example of how the best parameters are chosen is given using the [Optuna](https://optuna.org/) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ec1ed-f350-4de6-ba1d-678045baedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa937ce0-30c5-4eaa-aae8-9e9b4f3c93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bbc51-d5d4-47cc-9c6b-48cce9167f83",
   "metadata": {},
   "source": [
    "### The first dataset - Palmer Penguins!\n",
    "\n",
    "<img src=\"images/palmer_penguins.png\" width=\"600\">\n",
    "\n",
    "This dataset consists of 344 datapoints on 3 species of penguin. We are focussing on the following columns for this workshop:\n",
    "\n",
    "* Bill length (mm)\n",
    "* Flipper length (mm)\n",
    "* Bill depth (mm)\n",
    "* Body mass (g)\n",
    "\n",
    "The idea behind this is that we can build various model to use this information to predict which species of penguin the datapoints belong to. The model uses the labels in the existing dataset to build models that would be able to then classify new datapoints. These are all examples of [supervised learning](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce967bb9-4ab3-40cf-9655-542068167f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_raw = load_penguins()\n",
    "penguins = (\n",
    "    penguins_raw\n",
    "    .drop(columns=[\"island\", \"sex\", \"year\"])\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a994614-8645-4f5f-b48f-3dc617caa163",
   "metadata": {},
   "source": [
    "Note that 2 of these rows must have contained some missing data points for these columns. As this is such a low number, they are just removed from the analysis here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12773d81-4033-4f28-a3d7-a6f27f4bd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fcfca-bcde-4aa7-b447-24fab29bc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3cbd9-14b7-42f2-9f29-3a1fcf590fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edf1e6-08ba-4cf5-90fd-23bff15cd12f",
   "metadata": {},
   "source": [
    "### Visualising the data - Seaborn `pairplot`\n",
    "\n",
    "If you are dealing with a fairly small number of datapoints, then the [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function from the Seaborn library.\n",
    "\n",
    "This shows all of the scatter plots for pairwise combinations of columns, as well as the density plots of the individual features along the diagonal.\n",
    "\n",
    "What we are interested in finding here is the plot that shows the clearest separation between the different species. When just looking at this graph, it look as though this is done by comparing `flipper_length_mm` and `bill_length_mm`.\n",
    "\n",
    "Note that these plots are symmetric about the diagonal, so the graph of these 2 features occurs twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c885eaa-635f-4591-b7d6-e86525dcea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    penguins,\n",
    "    hue=\"species\",\n",
    "    height=2.5,\n",
    "    plot_kws={\"s\": 10}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa7ac6-51f5-49be-91a0-c81a13c1d531",
   "metadata": {},
   "source": [
    "## Using a JointPlot to focus on 2 features\n",
    "\n",
    "Another useful function from seaborn is the [jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html), which by default shows the scatter plot and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f007b1-d70d-4aa6-bb63-1d911786eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(\n",
    "    data=penguins,\n",
    "    x=\"flipper_length_mm\",\n",
    "    y=\"bill_length_mm\",\n",
    "    hue=\"species\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821166a-88e7-46a0-8b49-256795bed257",
   "metadata": {},
   "source": [
    "## Splitting by feature value\n",
    "\n",
    "One thing to note when using a decision tree is that the splits are made at the feature level. What this means is that, when looking at this 2D plot - the splits will occur in a perpendicular fashion to the axes. This means that the splits are either horizontal or vertical - we cannot get diagonal splits.\n",
    "\n",
    "What can be seen below is an attempt to find the best splits using visual judgement alone - using the following values:\n",
    "\n",
    "* `flipper_length_mm` - 206mm\n",
    "* `bill_length_mm` - 44mm\n",
    "\n",
    "These values are very similar to those found by the decision tree algorithm, which you will see a few cells below - and hopefully provides an intuition as to what the algorithm is looking to perform.\n",
    "\n",
    "In this first plot - all of the datapoints to the right of the red line can be classified as Gentoo - with only a few errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef628acd-eef7-4560-96da-334dece078d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(\n",
    "    data=penguins,\n",
    "    x=\"flipper_length_mm\",\n",
    "    y=\"bill_length_mm\",\n",
    "    hue=\"species\"\n",
    ")\n",
    "ax.ax_joint.axvline(206, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0cfaa-b65d-4f4e-a580-a00f113bd8ea",
   "metadata": {},
   "source": [
    "#### This next plot is only looking at the remaining data points\n",
    "\n",
    "Once again - the split is not perfect, but provides a very good generalisation to perform the remaining classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725fb40-4a20-483a-b4bf-5417a303b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(\n",
    "    data=penguins.loc[lambda x: x[\"flipper_length_mm\"] < 206],\n",
    "    x=\"flipper_length_mm\",\n",
    "    y=\"bill_length_mm\",\n",
    "    hue=\"species\"\n",
    ")\n",
    "ax.ax_joint.axhline(44, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60129a1f-7784-4ba5-99c6-2236e29a33ea",
   "metadata": {},
   "source": [
    "## Trying out some Decision Trees\n",
    "\n",
    "In this section, 2 different trees will be built with 1 and 2 splits respectively.\n",
    "\n",
    "If this is not clear now - then some later diagrams should help to clarify what is going on.\n",
    "\n",
    "In order to assess performance, we are making use of a procedure called \"cross validation\" (CV). This is used to ensure that a model's performance is always carried out on datapoints that __were not used when building the model__. This is such an important concept to remember, as the main interest when building a model is to assess how it will respond to new data points that it did not see when it was fitted. A more detailed description of this process can be read on [the scikit-learn website](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "Some key points to note:\n",
    "\n",
    "* The data is split into _K_ \"folds\", and each split gets to become the validation set once.\n",
    "    * So for 10-fold CV the following process occurs:\n",
    "        * The data is split into 10, with each split containing 10% of the data\n",
    "        * The model is trained on 90% of the data - with the performance assessed on the held out 10%\n",
    "            * This process is repeated 10 times\n",
    "* \"Stratified\" ensures that the proportions of each class is kept at each split\n",
    "* \"shuffle\" ensures that the whole dataset is shuffled before the splits occur\n",
    "    * This can be kept the same for comparison purposes by setting a \"seed\" value via the `random_state` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68516f51-fc6f-4840-aee8-4c009bce7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229fe81-b730-42bf-9cd8-545ba9a60d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins = penguins.drop(columns=\"species\")\n",
    "y_penguins = penguins[\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5734455-65b4-42af-994c-8e74a295ac47",
   "metadata": {},
   "source": [
    "### Note that the number of penguins in each species is not the same - this is why using a Stratified CV is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde49f6-cb44-4273-a67d-b9bc96847711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_penguins.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7be193-d50e-4d5e-ab30-ea7e31efae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_penguins.value_counts() / y_penguins.value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867599fd-b450-4ced-9ddb-bd27c73dc1bd",
   "metadata": {},
   "source": [
    "## Fitting the models\n",
    "\n",
    "Note that a decision tree with a depth of 1 can be called a __Decision Stump__\n",
    "\n",
    "This tree only has the freedom to perform __one__ split to try and get the best possible classifier. It is therefore no surprise that its performance is below 83% accuracy for all splits.\n",
    "\n",
    "### A note on performance metrics\n",
    "\n",
    "Choosing the best way of judging a model's performance can get very complicated. A full description of this process is outside of the scope of this workshop, and to keep things simple, this notebook is making use of the `accuracy` metric, which means that the model is judged solely on whether it make the correct classification. The process is binary for every validation datapoint and the score is given as the proportion of correctly classified datapoints.\n",
    "\n",
    "This metric is completely unsuitable if you have a very imbalanced dataset. For example, if there were only 2 species of penguin, with one of them accounting for <1% of the total number; then a model that just states that all penguins are of the majority species will have an accuracy value of >99% while also being completely useless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fd50a-6698-47e7-b56a-6aa12fb63205",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014b546-ef4e-4dfd-a4e2-8a27eb8dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1_scores = cross_val_score(tree1, X_penguins, y_penguins, scoring=\"accuracy\", cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681a1d3-1ce7-4fa2-8d95-ab708096553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54ec40-599d-41a1-8ed1-c85efa48415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec3239-5a07-489e-b1ec-db29ddf08dd3",
   "metadata": {},
   "source": [
    "### A tree with 2 splits\n",
    "\n",
    "This next tree is permitted to split the data again following the first split. This is very similar to the approach that was shown in the plots above and, not surprisingly, gets a much higher level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a122b-7f2e-4dc6-aa33-93ad04452ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666ac29-2e19-4510-8ddf-f9b7d6b452b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_scores = cross_val_score(tree2, X_penguins, y_penguins, scoring=\"accuracy\", cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074dd10-e9b8-4e20-b92e-2e71760c93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abe143-286a-4a88-b3b6-fc7b2e7d89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4eaa3-d2c7-4b54-bfb7-bd765841d41d",
   "metadata": {},
   "source": [
    "## Plotting the Trees\n",
    "\n",
    "The advantage of using single Decision Trees is that they are very easy to interpret. This can be seen when using the `plot_tree` function on a fitted model.\n",
    "\n",
    "Some terminology:\n",
    "\n",
    "* __Node__ - this is the visual representation of a block of datapoints - either before or after a split\n",
    "    * This can be seen as the rectangular boxes in the plots below\n",
    "* __Parent node__ - the node _before_ a split has occurred and is higher in the plot\n",
    "* __Child node__ - the node _after_ a split has occurred and is lower in the plot\n",
    "* __Branches__ - all of the paths flowing downward throughout the tree\n",
    "* __Leaf node__ - the final nodes at the ends of the branches:\n",
    "    * This is where the classification occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55968fb7-aadd-48d3-8e01-b377a478fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X_penguins, y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c5b9d-4d87-4599-80c8-1e8dc52fcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(tree1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86e78-370b-44b8-bda9-c010ff3b64d0",
   "metadata": {},
   "source": [
    "### Getting the column names\n",
    "\n",
    "Unfortunately - the plot does not show the column names, but we can use the index values and the columns from the original dataframe to obtain the feature that was used for the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b274e91-7197-4834-a596-4d1dca476813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins.columns[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37518e39-907c-4c91-b6d3-45f0850e5a93",
   "metadata": {},
   "source": [
    "## Majority rules\n",
    "\n",
    "Note that this decision stump has only been able to make a binary classification, as it was only able to perform a single split. The Gentoo penguins have been identified very well, but the remaining 2 can be separated. Because there are more Adelie penguins than Chinstrap, the model automatically assumes that all will be classified as Adelie, as this way the accuracy score will be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c224b-02e1-4333-bcab-89cfa44f05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tree1.predict(X_penguins))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50419f82-3614-4527-b1b5-8358326a7af8",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "\n",
    "This metric is the key to explaining how the decision tree makes its classifications. It can be thought of as \"how clear is the signal from this node?\". If a node following a split only contains values from 1 of the classes, then this message is 100% umambiguous... it is predicting that class with complete certainty. If however, the node contains equal numbers of each class, then it is impossible to tell which class is more likely.\n",
    "\n",
    "This ambiguity can be quantified using the __Gini Impurity__ measure:\n",
    "\n",
    "* A __lower__ value suggests __less__ ambiguity (with a minimum value of 0)\n",
    "* A __higher__ value suggests __more__ ambiguity (max value dependent on number of classes)\n",
    "\n",
    "#### The formula is as follows: (don't worry if this does not make sense)\n",
    "\n",
    "$\\Large 1 - \\sum_{i=1}^{n} (p_{i})^{2}$\n",
    "\n",
    "Here we create arrays of probabilities for class A and B. This represents the situations when a leaf node is fully populated with category A $p(A) = 1$ or category B $p(B) = 1$, and all combinations inbetween.\n",
    "\n",
    "We can see the the impurity value is 0 when the probability of A is either 1 or 0 - in the latter case because we can be 100% sure of class B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b02ae-adc8-4a5a-a691-fc619cb6b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_a = np.linspace(0, 1, 1000)\n",
    "prob_b = 1 - prob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1b159-ac35-422e-a482-2cf590087957",
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_results = 1 - (prob_a**2 + prob_b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7e3d8-3dbc-4681-b1fb-91fe5b0f0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(prob_a, gini_results)\n",
    "ax.set_xlabel(\"Probability of A\")\n",
    "ax.set_ylabel(\"Gini Impurity Value\")\n",
    "plt.suptitle(\"Gini Impurity for Binary Classification\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b0cf9-0c32-4327-ac8a-bd3c9844fa39",
   "metadata": {},
   "source": [
    "Note that when using more classes - the impurity value can go greater than the 0.5 level that is suggested in the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4890270-cefb-4ddf-88cd-3a25daf6bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.333333**2 + 0.333333**2 + 0.33333**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad5ee7-489f-45c8-a88a-44c2a3fc6776",
   "metadata": {},
   "source": [
    "## Information Entropy\n",
    "\n",
    "Another criterion measure that can be used to the same effect is Information or __Shannon's__ entropy. It has the following formula (using a logarithm of base 2):\n",
    "\n",
    "$\\Large - \\sum_{i=1}^{n} p_{i} \\cdot log_{2}(p_{i})$ \n",
    "\n",
    "Note that the __dot__ here represents multiplication.\n",
    "\n",
    "This is a perfectly reasonable alternative - but it is computationally slower due to the calculation of the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffc9e2-bb81-40ee-8af0-5b5d57c78060",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_results = -(prob_a[1:] * np.log2(prob_a[1:]) + prob_b[:-1] * np.log2(prob_b[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71191210-7fd5-4e7d-9ed2-46cf840a9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(prob_a[1:], entropy_results)\n",
    "ax.set_xlabel(\"Probability of A\")\n",
    "ax.set_ylabel(\"Entropy Value\")\n",
    "plt.suptitle(\"Entropy for Binary Classification\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5c633-aa82-497c-9ce6-2934fa3e86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.criterion = \"entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9195779-e55a-4cce-a8a6-b25c318bf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X_penguins, y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21882b-8f18-4c1d-8469-7a1307a9cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(tree1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ac097-7df5-43eb-b157-947f307adb7b",
   "metadata": {},
   "source": [
    "## Looking at a tree with a depth of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84a180-0dc0-44b2-8f42-e33856d8382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.fit(X_penguins, y_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e53026-0a6b-4c22-9dd9-b37118352deb",
   "metadata": {},
   "source": [
    "### Note that the 3rd leaf node achieves a Gini score of 0 - meaning no impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b130ec5-4ba6-4893-9d90-8fcda4c64b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_tree(tree2, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18926b07-e968-441b-9186-94ce05af5819",
   "metadata": {},
   "source": [
    "### Looking at the information on the 2 left branches\n",
    "\n",
    "This is what we saw earlier, and gets very good classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbaafb-765b-484f-8c16-20019cf9c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bfc52-464d-4767-b6f1-fffbe409b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f724e3-5374-45d5-8e75-03b34e204562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(\n",
    "    data=penguins,\n",
    "    x=\"flipper_length_mm\",\n",
    "    y=\"bill_length_mm\",\n",
    "    hue=\"species\"\n",
    ")\n",
    "\n",
    "ax.ax_joint.axvline(206.5, c='r')\n",
    "ax.ax_joint.axhline(43.35, c='r', xmax=0.555);  # This just took experimentation to get the xmax value correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d7267-96e2-4e5b-9a56-81a2cde61ba4",
   "metadata": {},
   "source": [
    "## The right hand path looks a bit different - is the second split really necessary?\n",
    "\n",
    "Also note that the top left area contains data points for Gentoo penguins __only__ - this is why we are getting a Gini value of 0 for this leaf node.\n",
    "\n",
    "Also - notice that there is still 2 of the Gentoo penguins outside of this classification region.\n",
    "\n",
    "__A zero Gini score does NOT mean that we have captured ALL of a particular class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff5a8e-636e-45f5-a17f-b34286b032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b8e1e-d29c-4b47-9b87-fbdd94389bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(\n",
    "    data=penguins,\n",
    "    x=\"bill_depth_mm\",\n",
    "    y=\"flipper_length_mm\",\n",
    "    hue=\"species\"\n",
    ")\n",
    "\n",
    "ax.ax_joint.axvline(17.65, c='r', ymin=0.56)\n",
    "ax.ax_joint.axhline(206.5, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143c4c8-29c9-4e74-b8cf-c4bf1b5292b0",
   "metadata": {},
   "source": [
    "### Is this last split really necessary?\n",
    "\n",
    "The split in the top right hand corner has only identified 7 data points. While it can be argued that it is necessary here... it could be _specific to the data used to train the model_. This is a good example of what could very likely be a case of __Overfitting__. This is when we make a model that is too tailored to the particular data points used to train the model.\n",
    "\n",
    "#### This is a common problem that can occur when using a single decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1b9d6-c26a-4130-b0ce-1c3099ee9323",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "The next stage is not to just use 1 Decision Tree - but build lots of them in order to get a concensus decision!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8fac3-6471-4bb7-a414-bd69c4716bff",
   "metadata": {},
   "source": [
    "## Wine Quality Dataset\n",
    "\n",
    "This is a more complex dataset, but is great to illustrate the power of ensemble methods.\n",
    "\n",
    "The dataset consists of 178 datapoints of different wines, which have been classified into 3 different types, labelled 1, 2, and 3. Each wine contains information on 13 various characteristics, and the task of the model is again a multi-classfication task using this labelled data.\n",
    "\n",
    "Note that this dataset is now getting too large to be able to use the pair plot function effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd85a8-bcc3-4dce-9bb9-52d1b9d758ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_raw = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8f9d5-d1c4-4e5c-b2e3-a96410821b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.DataFrame(\n",
    "    data=wine_raw[\"data\"],\n",
    "    columns=wine_raw[\"feature_names\"]\n",
    ")\n",
    "\n",
    "full_wine_df = (\n",
    "    wine_df\n",
    "    .assign(**{\n",
    "        \"Type\": wine_raw[\"target\"].astype(str)\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174db68-5a99-4b83-b393-446b014b56ac",
   "metadata": {},
   "source": [
    "## Slight imbalance again\n",
    "\n",
    "We will look at a way that this can dealt with once we start looking at Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595e68f-8403-4c55-9001-1b6493eebb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wine_df[\"Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4de04e-062c-48e2-bdd9-588574f3747c",
   "metadata": {},
   "source": [
    "## Fitting the Models - both single trees and ensembles\n",
    "\n",
    "The work here is heavily based on a fantastic blog by Frank Ceballos, which can be read [here](https://towardsdatascience.com/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b).\n",
    "\n",
    "The main idea here is that are to do the following\n",
    "\n",
    "* Create a decision stump\n",
    "* See how well this performs on the wine data\n",
    "* Create different ensembles of this stump to try and get better performance - note that the final 2 require small changes to the decision tree parameters:\n",
    "    * 1 with a deliberate error\n",
    "    * 1 that works via bootstrap sampling\n",
    "    * 1 that works in the manner of a __Random Forest model__\n",
    "    * 1 that works in the manner of an __Extremely Randomised Trees model__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a44197-7d11-47cb-8684-cd1b9b5d7d2e",
   "metadata": {},
   "source": [
    "## Incorrect Bagging\n",
    "\n",
    "The following cells show the first attempt at an ensemble.\n",
    "\n",
    "First we create the Decision stump in the same manner as before. In order to get a baseline - we will use the [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) from scikit-learn. This model just produces predictions at random, and shows the result of a model that performs purely at chance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e9d7d-a4bc-40d1-9d7f-62f2adb625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine = wine_df.copy()\n",
    "y_wine = full_wine_df[\"Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6701e-91aa-4207-b7c9-c31c640953dc",
   "metadata": {},
   "source": [
    "The results from the dummy classifier show expected poor results - note that with 3 classes that do not show massive levels of imbalance, making predictions based solely on chance would intuitively result in a score of c.$\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce8ad6-10b9-4261-9b76-e1eba52285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66fc20-9252-459c-8069-c2be8ec6583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(dummy, X_wine, y_wine, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7f79f-0d29-42fc-b317-25b497d9733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_tree = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e496ad3-9464-477c-8210-9151c37dd4a6",
   "metadata": {},
   "source": [
    "#### Slightly better performance\n",
    "\n",
    "This is expected, as the single tree has at least _started_ to segregate the data - but it is not doing enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0f475-f1be-4521-8da0-78ab5f9a4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(wine_tree, X_wine, y_wine, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783175dd-76bb-438a-bd04-82f3d8c1dbd7",
   "metadata": {},
   "source": [
    "## Creating the ensemble\n",
    "\n",
    "Note the parameters:\n",
    "\n",
    "* `base_estimator` - The type of decision tree that will be used multiple times\n",
    "* `n_estimators` - The number of trees that will be built\n",
    "* `bootstrap` - More on this later!\n",
    "* `n_jobs` - How many jobs that you want to run in parallel\n",
    "\n",
    "### Parallel computation!\n",
    "\n",
    "A wonderful feature of these trees is that each can be trained _independently_ of each other. This means that we can make use of the multiple cores on the computer to train the models in an _embarrassingly parallel_ manner.\n",
    "\n",
    "If you want to use as much parallelisation as possible, but don't know how many cores you have - just set this value to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36fb55-0241-4002-a15f-dd4f7ced0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_bagging_bad = BaggingClassifier(\n",
    "    base_estimator=wine_tree,\n",
    "    n_estimators=100,\n",
    "    bootstrap=False,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31382c-0d8b-4461-aa7f-6447d9e86ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(wine_bagging_bad, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c31bc4-728f-46e1-80b4-ae45f7fcf868",
   "metadata": {},
   "source": [
    "## What??!!!!!!1\n",
    "\n",
    "... is going on here? Wasn't this supposed to be better??\n",
    "\n",
    "So - what is happening here is that we are effectively building __exact replicas__ of the same tree. As Frank Ceballos says - this is like asking the same person what their favourite movie is multiple times - you're going to get the same answer every time!\n",
    "\n",
    "The trick here is in the `bootstrap` parameter - we need to set it to `True`\n",
    "\n",
    "### What is bootstrapping?\n",
    "\n",
    "In short - the term refers to __sampling with replacement__. So - if we have 100 data points - we don't just take all of them, we _sample_ 100 of them, with each datapoint having the chance to be selected multiple times.\n",
    "\n",
    "The idea behind this is that each tree will not get the same datapoints, but the _sampled_ datapoints.\n",
    "\n",
    "Another thing to remember when doing this resampling is that you are more likely to sample those that tend to cluster together, and _less_ likely to get the outliers. Of course - some of the trees will get the outliers - but remember that the result is taken as the _aggregate performance_, so on average, the outliers will have less of an effect on the final outcome.\n",
    "\n",
    "It really seems as though this simple change of doing the sampling would not make much difference - but in fact it makes _all the difference!_.\n",
    "\n",
    "This is a perfect example of how these models are __Ensembles of weak learners__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e5770-324b-4c51-9cc4-197f2ba6b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_bagging = BaggingClassifier(\n",
    "    base_estimator=wine_tree,\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,\n",
    "    random_state=456, # This is to ensure that the bootstrapping is the same\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdf417-e13a-4c1e-ab07-2c8724ffc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(wine_bagging, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe62fe-2f00-42bc-a327-aa7894440883",
   "metadata": {},
   "source": [
    "## Ensemble like a Random Forest\n",
    "\n",
    "The following shows how the bagging occurs in a Random Forest:\n",
    "\n",
    "* Each tree does __not__ get all of the features to use!\n",
    "    * Instead - it has to work with a limited set\n",
    "    * You can specify the exact number - but a common strategy is to use the square root or the log-2 value of the total number\n",
    "    * Another this is the `splitter` parameter - we will discuss this more in the next section\n",
    "        * Here we explicitly set it as `best` - but this is the default anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e25958-8835-4b95-ad87-8990cba22571",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_tree_2 = DecisionTreeClassifier(\n",
    "    max_depth=1,\n",
    "    max_features=\"sqrt\",\n",
    "    splitter=\"best\",\n",
    "    random_state=753  # this is needed due to the random nature of the feature selection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2975d-7dac-407b-9d70-0d6b30e38fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_bagging_2 = BaggingClassifier(\n",
    "    base_estimator=wine_tree_2,\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,\n",
    "    random_state=456,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e867161-9399-41f6-9afa-fb13c6131a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(wine_bagging_2, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3782b-2e9a-431e-9dee-2f7c2549a1d9",
   "metadata": {},
   "source": [
    "## Why is this so much better?\n",
    "\n",
    "This result shows that there _must_ be some variables that contain more useful information to allow the model to discriminate between the classes. Some of the features of the wine might not be of much use at all.\n",
    "\n",
    "By only giving the different trees subsets of the variables, there will be cases where some trees get the _less_ useful ones, and others get the _more_ useful ones. Those with the better features will do a better job of classification, with lower Gini values in their leaf nodes. When we take the final aggregated outcome, these trees are sending a robust and cohesive signal, whereas the trees that had to work with the worse features will be more confused. __The majority signal will win on aggregate__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45781cdb-784f-430c-9fcc-a8cbb2ae5f9b",
   "metadata": {},
   "source": [
    "## Ensemble like an Extremely Randomised Trees Classifier\n",
    "\n",
    "There are 2 key differences here:\n",
    "\n",
    "* We do not bootstrap (by default when using the actual module from scikit-learn - you can if you want to!)\n",
    "* We use a random splitter\n",
    "\n",
    "#### What is meant by the splitter?\n",
    "\n",
    "So this was the bit that confused me the most when I was learning the difference between these 2 algorithms; so I will do my best to summarise here:\n",
    "\n",
    "* The best method performs a thorough scan of all the features when building the trees - calculating the Gini (or entropy) at all the stages\n",
    "    * This gets the _best_ possible split\n",
    "    * But... it is computationally expensive!\n",
    "* A random method will instead pick a number of random splits to check (all different at each split and for the different trees) - and __not__ do a full scan\n",
    "    * This won't necessarily find the best split overall - but will return the _best split of those that it has checked_\n",
    "    * The idea being that if this is done across enough trees - we will still get a very powerful answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e5c66-2e9c-44ff-b6c5-7216be2db183",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_tree_3 = DecisionTreeClassifier(\n",
    "    max_depth=1,\n",
    "    max_features=\"sqrt\",\n",
    "    splitter=\"random\",\n",
    "    random_state=753  # This will also fix the randomness for the splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613c8d0-a42d-42f1-8da4-269d8dc1945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_bagging_3 = BaggingClassifier(\n",
    "    base_estimator=wine_tree_3,\n",
    "    n_estimators=100,\n",
    "    bootstrap=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=456\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f3125-4ed3-40c1-8045-4d456206b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(wine_bagging_3, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5a855-01af-4269-9132-eb648dee42db",
   "metadata": {},
   "source": [
    "## This isn't doing as well - why not just use the BEST splitter? Isn't \"best\" always best?\n",
    "\n",
    "This was the bit that confused me the most when learning about this. But it does actually make sense when you think about what can happen with different datasets.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "This algorithm will try its hardest to find the best set in all of the features given to each tree. While this is fantastic if there is a clear signal, it can actually be a hindrance if the trees have any features that contain no (or very little) signal, as they will _still_ try to find what they can. What this will result in is the model __trying to find signal in noise__, as it will find any bespoke signal in the training data and _assume_ that this can be generalised to new data points. This is the typical example of __what leads to overfitting__.\n",
    "\n",
    "In fact, I have read that Random Forests, while often being the first algorithm that researchers will try, will often require some form of feature selection method, used in addition to the fitting of the model in a pipeline. More about this can be read in the [feature-selection](https://scikit-learn.org/stable/modules/feature_selection.html) and [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) sections of the scikit-learn documentation.\n",
    "\n",
    "#### Extremely Randomised Trees\n",
    "\n",
    "These, on the other hand are not so hampered by noise variables, as they will not expend so much effort trying to find non-existent best splits in noisy features. They will select some random splits, see what happens, and then move on.\n",
    "\n",
    "As for the bootstrapping - it is True by default for Random Forests and False for Extremely Randomised Trees. This can be changed by setting the `bootstrap` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c0b42-ca59-45eb-bb5d-c50e813fcb76",
   "metadata": {},
   "source": [
    "# Using these models directly\n",
    "\n",
    "Note that these models will get better performance, as the trees used to make them are no longer restricted to being stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16edfc-0136-488e-a6fe-b82df6430ee8",
   "metadata": {},
   "source": [
    "## [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Click on the title to see the scikit-learn documentation.\n",
    "\n",
    "As can be seen - with the default parameters, we are getting amazing performance, with 7 out of the 10 splits achieving perfect classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043f329-0b27-4106-a228-04a208a2345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f109c0-bf5e-4b4c-8b1b-f1f80de80e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(random_forest, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d350c96-3f98-438b-87f7-507dbbaef325",
   "metadata": {},
   "source": [
    "## [Extra Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "\n",
    "Click on the title to see the scikit-learn documentation.\n",
    "\n",
    "Note that this is doing even better with 8 of the 10 splits having perfect classification! Showing that the \"best\" split doesn't necessarily mean \"best\" performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be9498-077d-4f1f-b1e3-c26823f38217",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees = ExtraTreesClassifier(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab46930-3355-463f-910b-e8e05041f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(extra_trees, X_wine, y_wine, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4fbd-f9cf-4bea-b3cf-555796af17b6",
   "metadata": {},
   "source": [
    "### Showing the time difference with `%%timeit`\n",
    "\n",
    "This is a useful \"magic\" function that can be used in Jupyter notebooks. It runs the command several times in order to get a distribution of the compute time required.\n",
    "\n",
    "As can be see, the Extra Trees model runs in 70-80% of the time (or better) of the Random Forest. Note that this will vary on each run and might be different on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cde9aa-91cb-42a0-b6e3-711dd720f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_random_forest = RandomForestClassifier(random_state=345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97249dc5-ca48-4c93-8010-34279bfdf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "timer_random_forest.fit(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5eab0-6196-4083-91bf-f665796342df",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_extra_trees = ExtraTreesClassifier(random_state=345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fac00-2028-4cbe-88e7-033118a4d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "timer_extra_trees.fit(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d77469-8566-41be-ac55-f0b060301639",
   "metadata": {},
   "source": [
    "## A note on imbalanced classes and setting the `class_weight`\n",
    "\n",
    "One interesting parameter that can be set in all of the algorithms used so far is `class_weight`.\n",
    "\n",
    "This can be very useful in situations when you don't have equal representation of the different classes that you are trying to predict. It can be argued that we could have used it in this notebook so far.\n",
    "\n",
    "### Weighted Classes\n",
    "\n",
    "A full description of how this is done will not be discussed here, but the idea is as follows:\n",
    "\n",
    "* The classes with lower representation are given a higher weight\n",
    "* These datapoints then contribute __more__ towards a higher Gini Impurity than the lower weighted classes\n",
    "* This has the effect that the splits start to favour trying to classify these _minority_ classes with higher accuracy\n",
    "* The idea is that this counteracts the type of behaviour that we saw earlier with the Chinstrap penguins\n",
    "* A good strategy is to use `class_weight=\"balanced\"` when building the model, as this weights each classes by their __inverse proportion__\n",
    "    * This means that those classes with fewer data points get a higher weighting\n",
    "    * You can also choose `class_weight=\"balanced_subsample\"` to change the weights based on the proportions in each split\n",
    "        * But this should not be an issue when using Stratified splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d216679-a385-460c-b94a-a8039a036cab",
   "metadata": {},
   "source": [
    "## A note on feature importance\n",
    "\n",
    "While the ensemble methods are, in general, far superior at classification prediction, a big disadvantage is that they are not as interpretable, as we cannot just simply see how a single tree is traversed.\n",
    "\n",
    "However, what we can do with a fitted model is look at the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172190d-579f-442f-924d-0eee1a191fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_wine_importance = timer_random_forest.feature_importances_\n",
    "\n",
    "wine_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_wine.columns,\n",
    "    \"Importance\": rf_wine_importance\n",
    "}).sort_values(\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4aa02-be20-4c8e-b324-20ee980afcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fc91b-5139-41c9-ae9b-2bf9d474293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features = list(wine_importance_df[\"Feature\"].values[:5]) + [\"Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8801704-9a6b-4d28-83e5-1245e38905a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17189166-a2ac-49a5-a997-6a79453a81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=full_wine_df.loc[:, top_5_features],\n",
    "    hue=\"Type\",\n",
    "    height=2.5,\n",
    "    plot_kws={'s': 10}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092b124-3981-4de0-85ac-a6e04c00879f",
   "metadata": {},
   "source": [
    "# Hyper-parameters and their tuning\n",
    "\n",
    "## Random Forest hyper-parameters - more can be seen in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "It should now be clear what these hyperparameters are doing and how they affect the building of the trees in the ensemble. Note that many of them are put in place to actually restrict the freedom that each tree has to a greater or lesser degree. This is vital when dealing with data sets that are not as clear cut as the examples that we have seen, and we need to make sure that the models do not overfit to noise in the training data.\n",
    "\n",
    "Note that this is not the full this, and there are some parameters in the documentation that we have not discussed due to time. These will not be listed here, and are not required for the final task.\n",
    "\n",
    "* `n_estimators` - Integer: The number of trees to build - default 100\n",
    "* `criterion` - String: 'gini' or 'entropy' or 'log_loss' (which we haven't discussed here) - default 'gini'\n",
    "* `max_depth` - Integer: Sets how deep the tree can go - default None = no limit\n",
    "* `min_samples_split` - Integer or Float: How many datapoints a node must if it is allowed to be split further (proportion of total if float) - default 2\n",
    "* `min_samples_leaf` - Integer or Float: How many datapoints must remain in a leaf node (proportion of total if float) - default 1\n",
    "* `min_weighted_fraction_leaf` - Float: The minimum weighted fraction of the datapoints in a leaf as a proportion of the entire original set - default 0\n",
    "    * Each datapoint is equally weighted if no `class_weight` is set\n",
    "    * This parameter is a bit more complicated to understand - but is another way of stopping a leaf node from having too few datapoints\n",
    "* `max_features` - Integer, Float or String: Either specify the exact number, the proportion - or use one of the built in strings like 'sqrt' - default 'sqrt'\n",
    "    * see docs for more info\n",
    "* `max_leaf_nodes` - Integer: How many leaf nodes are permitted per tree - default None = no limit\n",
    "* `min_impurity_decrease` - Float: The reduction in impurity that must be seen for a split to occur - default 0\n",
    "    * Simply put - \"is this split really worth it? Does it bring any real benefit?\"\n",
    "* `max_samples` - Integer or Float - defaults to the original sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911d6dd-fff7-4035-82eb-d9f30b53f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_raw = pd.read_csv(\"https://raw.githubusercontent.com/pydatacardiff/pydata_cardiff_workshop4/main/data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f624457-2361-482f-b91c-9c00096240bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_heart = heart_raw.drop(columns=\"output\")\n",
    "y_heart = heart_raw[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb3e56-424c-4afa-b49a-6b72570bf3f9",
   "metadata": {},
   "source": [
    "Note that this a pretty balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ae5b7-9cb1-44c9-bad1-3933d48a01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_heart.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710c9e8-571d-4204-9f0e-2c10b6f92abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_heart = RandomForestClassifier(random_state=123)\n",
    "extra_trees_heart = ExtraTreesClassifier(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ace07-0d3d-4a75-b7e0-6b9ede49b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(random_forest_heart, X_heart, y_heart, cv=cv, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263162f-8602-4570-9959-3c1fbffff90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(extra_trees_heart, X_heart, y_heart, cv=cv, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc95481-0d5a-45f3-ae88-168e604069fd",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1c579-9806-4d7f-9059-9e2552f52d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51b121-c4d6-44fd-8e27-025a8352cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # This will check both Random Forests and Extra Trees\n",
    "    classifier_type = trial.suggest_categorical(\"classifier\", [\"RandomForest\", \"ExtraTrees\"])\n",
    "    \n",
    "    # Setting the distributions for the parameters - sticking to common parameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 1000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 200, log=True)\n",
    "    min_samples_split = trial.suggest_float(\"min_samples_split\", 0, 1)\n",
    "    min_samples_leaf = trial.suggest_float(\"min_samples_leaf\", 0, 0.5\n",
    "                                          )\n",
    "    \n",
    "    if classifier_type == \"RandomForest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=123,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=123,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    cv_scores = cross_val_score(model, X_heart, y_heart, cv=cv, n_jobs=-1, scoring=\"accuracy\")\n",
    "    \n",
    "    score = cv_scores.mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb7961-1f58-428f-a549-f8cea9571b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=\"ensemble_study\", direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc3b7c-ecfa-4d8f-bbba-c1025c2273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620443b0-f367-43a4-91cb-247e55a2b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f36e4-aa5a-4a0d-a369-1142dbe0fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
